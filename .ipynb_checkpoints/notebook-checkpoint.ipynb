{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e9df97-c2e2-4977-b7ce-0d4aab11fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93b61be-cea4-4716-945e-c148345003c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad1 = pd.read_parquet(\"ad1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f44ad35-a1ed-43d2-bd2a-93f1d9f892d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad2 = pd.read_parquet(\"ad2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4411f92-2b25-4fb7-a133-8b28bc30c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "anomalies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9666bc1-5acc-4fa5-b85a-33f71a317046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalous_price_change(df, anomalies, threshold=0.05, resample_freq='1S'):\n",
    "    \n",
    "    instruments = [inst for l1, inst in df.columns if l1 == 'ltp']\n",
    "\n",
    "    for inst in instruments:\n",
    "        try:\n",
    "            # Extract and clean the LTP series\n",
    "            ltp_series = df[('ltp', inst)].replace(0, np.nan).dropna()\n",
    "\n",
    "            # Resample to uniform interval if needed (only if not already uniform)\n",
    "            ltp_series = ltp_series.resample(resample_freq).ffill().dropna()\n",
    "\n",
    "            if len(ltp_series) < 2:\n",
    "                continue\n",
    "\n",
    "            # Compute percentage change\n",
    "            price_change_rate = ltp_series.pct_change().replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "            # Detect anomalies\n",
    "            anomaly_times = price_change_rate[price_change_rate.abs() > threshold].index\n",
    "\n",
    "            for ts in anomaly_times:\n",
    "                anomalies.append({\n",
    "                    'datetime': ts,\n",
    "                    'stock_id': inst,\n",
    "                    'anomaly_type': 'Anomalous Price Change'\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log or skip issues gracefully\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72fd405-bc6b-4f4d-bc13-49744a70fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "def detect_anomalous_volume_zscore(df, anomalies, threshold=3):\n",
    "    # Identify instruments with 'traded_volume' data\n",
    "    instruments = [inst for l1, inst in df.columns if l1 == 'traded_volume']\n",
    "\n",
    "    for inst in instruments:\n",
    "        try:\n",
    "            # Extract and clean the traded volume series\n",
    "            volume_series = df[('traded_volume', inst)].replace(0, np.nan).dropna()\n",
    "\n",
    "            if len(volume_series) < 2:\n",
    "                continue\n",
    "\n",
    "            # Ensure continuity by only analyzing segments without large gaps\n",
    "            volume_series = volume_series[volume_series.notna()]\n",
    "\n",
    "            # Compute z-score (standard score)\n",
    "            volume_z = zscore(volume_series, nan_policy='omit')\n",
    "\n",
    "            # Filter index where z-score is above threshold\n",
    "            anomaly_times = volume_series.index[np.abs(volume_z) > threshold]\n",
    "\n",
    "            for ts in anomaly_times:\n",
    "                anomalies.append({\n",
    "                    'datetime': ts,\n",
    "                    'stock_id': inst,\n",
    "                    'anomaly_type': 'Volume Spike Z-Score'\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue  # Gracefully skip issues in structure or calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8b2c857-fda8-43be-bd6f-6d073e6bc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bid_ask_spread_spike(df, anomalies, threshold=0.05):\n",
    "    # Identify instruments having both bid and offer\n",
    "    instruments = set(inst for l1, inst in df.columns if l1 in ('best_bid', 'best_offer'))\n",
    "\n",
    "    for inst in instruments:\n",
    "        try:\n",
    "            bid_series = df[('best_bid', inst)].replace(0, np.nan).dropna()\n",
    "            ask_series = df[('best_offer', inst)].replace(0, np.nan).dropna()\n",
    "\n",
    "            # Align on common valid timestamps\n",
    "            common_index = bid_series.index.intersection(ask_series.index)\n",
    "            bid_series = bid_series.loc[common_index]\n",
    "            ask_series = ask_series.loc[common_index]\n",
    "\n",
    "            # Remove zero or negative bid/ask prices (invalid)\n",
    "            valid_mask = (bid_series > 0) & (ask_series > 0)\n",
    "            bid_series = bid_series[valid_mask]\n",
    "            ask_series = ask_series[valid_mask]\n",
    "\n",
    "            if len(bid_series) < 2 or len(ask_series) < 2:\n",
    "                continue\n",
    "\n",
    "            # Compute mid-price and spread\n",
    "            mid_price = (bid_series + ask_series) / 2\n",
    "            spread = ask_series - bid_series\n",
    "\n",
    "            # Avoid division by zero or inf\n",
    "            spread_ratio = (spread / mid_price).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "            # Find times with anomalous spread ratios\n",
    "            anomaly_times = spread_ratio[spread_ratio > threshold].index\n",
    "\n",
    "            for ts in anomaly_times:\n",
    "                anomalies.append({\n",
    "                    'datetime': ts,\n",
    "                    'stock_id': inst,\n",
    "                    'anomaly_type': 'Bid-Ask Spread Spike'\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue  # Gracefully skip instruments with structural issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d72fdc95-1c92-4883-986a-f13db30e24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_delta_flip(df, anomalies, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Detects flips in Delta from positive to negative or vice versa,\n",
    "    excluding noise around zero using a small threshold (epsilon).\n",
    "    \"\"\"\n",
    "\n",
    "    instruments = [inst for l1, inst in df.columns if l1 == 'delta']\n",
    "\n",
    "    for inst in instruments:\n",
    "        try:\n",
    "            delta = df[('delta', inst)].replace(0, np.nan).dropna()\n",
    "\n",
    "            if len(delta) < 2:\n",
    "                continue\n",
    "\n",
    "            # Smooth out small noise around zero using epsilon\n",
    "            delta_filtered = delta.copy()\n",
    "            delta_filtered[np.abs(delta_filtered) < epsilon] = np.nan\n",
    "            delta_filtered = delta_filtered.fillna(method='ffill').dropna()\n",
    "\n",
    "            # Detect sign change\n",
    "            sign_change = np.sign(delta_filtered).diff().fillna(0)\n",
    "            flip_times = sign_change[sign_change != 0].index\n",
    "\n",
    "            for ts in flip_times:\n",
    "                anomalies.append({\n",
    "                    'datetime': ts,\n",
    "                    'stock_id': inst,\n",
    "                    'anomaly_type': 'Delta Flip'\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "637f8d32-0a89-4900-83f6-1a2e4a4d1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_vega_sensitivity_spike(df, anomalies, z_threshold=3, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Detects Vega sensitivity spikes using Z-score.\n",
    "    Handles noisy or missing data robustly.\n",
    "    \"\"\"\n",
    "    from scipy.stats import zscore\n",
    "\n",
    "    instruments = [inst for l1, inst in df.columns if l1 == 'vega']\n",
    "\n",
    "    for inst in instruments:\n",
    "        try:\n",
    "            vega_series = df[('vega', inst)].replace(0, np.nan).dropna()\n",
    "\n",
    "            if len(vega_series) < 10 or vega_series.std() < epsilon:\n",
    "                continue  # skip if not enough variation or too few points\n",
    "\n",
    "            z_scores = zscore(vega_series, nan_policy='omit')\n",
    "            anomaly_times = vega_series.index[np.abs(z_scores) > z_threshold]\n",
    "\n",
    "            for ts in anomaly_times:\n",
    "                anomalies.append({\n",
    "                    'datetime': ts,\n",
    "                    'stock_id': inst,\n",
    "                    'anomaly_type': 'Vega Sensitivity Spike'\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52214ad4-3e0c-4ffa-9fa6-5261cdd10f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_iv_bid_ask_divergence(df, anomalies, threshold=0.15, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Detects divergence between IV and midpoint of Bid IV and Ask IV.\n",
    "    Handles sparse, zero, or noisy data robustly.\n",
    "    \"\"\"\n",
    "    # Identify instruments having all three columns: iv, bid_iv, ask_iv\n",
    "    props, stocks = df.columns.levels\n",
    "    instruments = [stock for stock in stocks if all((prop, stock) in df.columns for prop in ['iv', 'bid_iv', 'ask_iv'])]\n",
    "\n",
    "    for inst in instruments:\n",
    "        try:\n",
    "            iv = df[('iv', inst)].replace(0, np.nan).dropna()\n",
    "            bid_iv = df[('bid_iv', inst)].replace(0, np.nan).dropna()\n",
    "            ask_iv = df[('ask_iv', inst)].replace(0, np.nan).dropna()\n",
    "\n",
    "            # Common time indices\n",
    "            common_index = iv.index.intersection(bid_iv.index).intersection(ask_iv.index)\n",
    "            if len(common_index) < 10:\n",
    "                continue\n",
    "\n",
    "            # Align and calculate mid-IV\n",
    "            mid_iv = (bid_iv.loc[common_index] + ask_iv.loc[common_index]) / 2\n",
    "            valid_mask = mid_iv > epsilon\n",
    "\n",
    "            # Avoid dividing by near-zero mid IV\n",
    "            if valid_mask.sum() < 5:\n",
    "                continue\n",
    "\n",
    "            divergence = ((iv.loc[common_index] - mid_iv) / mid_iv).abs()\n",
    "            divergence = divergence[valid_mask]\n",
    "\n",
    "            anomaly_times = divergence[divergence > threshold].index\n",
    "\n",
    "            for ts in anomaly_times:\n",
    "                anomalies.append({\n",
    "                    'datetime': ts,\n",
    "                    'stock_id': inst,\n",
    "                    'anomaly_type': 'IV-BidIV-AskIV Divergence'\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce517ad-854c-416b-b8d3-ce36917f53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "def detect_iv_spike(df, anomalies, z_threshold=3):\n",
    "    \"\"\"\n",
    "    Detects Implied Volatility (IV) spikes using Z-score method.\n",
    "    Filters out invalid values and handles sparse or non-continuous data.\n",
    "    \"\"\"\n",
    "    instruments = [inst for l1, inst in df.columns if l1 == 'iv']\n",
    "\n",
    "    for inst in instruments:\n",
    "        try:\n",
    "            iv_series = df[('iv', inst)].replace(0, np.nan).dropna()\n",
    "\n",
    "            # Remove negative or very small values\n",
    "            iv_series = iv_series[iv_series > 0]\n",
    "\n",
    "            # Ensure we have enough data for Z-score\n",
    "            if len(iv_series) < 10:\n",
    "                continue\n",
    "\n",
    "            # Calculate Z-score\n",
    "            iv_z = zscore(iv_series, nan_policy='omit')\n",
    "\n",
    "            # Identify anomaly timestamps\n",
    "            anomaly_times = iv_series.index[np.abs(iv_z) > z_threshold]\n",
    "\n",
    "            for ts in anomaly_times:\n",
    "                anomalies.append({\n",
    "                    'datetime': ts,\n",
    "                    'stock_id': inst,\n",
    "                    'anomaly_type': 'IV Spike'\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue  # Handle unexpected structure or NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13de7532-5d38-461a-b7a0-12d235370a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_anomalies(df, output_file='result.csv'):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    anomalies = []\n",
    "\n",
    "    # Call all anomaly detection functions\n",
    "    detect_iv_bid_ask_divergence(df, anomalies, threshold=0.15)\n",
    "    detect_anomalous_price_change(df, anomalies)\n",
    "    detect_anomalous_volume_zscore(df, anomalies)\n",
    "    detect_iv_spike(df, anomalies)\n",
    "    detect_vega_sensitivity_spike(df, anomalies)\n",
    "    detect_delta_flip(df, anomalies)\n",
    "    detect_bid_ask_spread_spike(df, anomalies, threshold=0.05)\n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(anomalies)\n",
    "\n",
    "    if not result_df.empty:\n",
    "        result_df.sort_values(by=\"datetime\", inplace=True)\n",
    "        result_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(result_df)} anomalies to {output_file}\")\n",
    "    else:\n",
    "        print(\"No anomalies detected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "195d8b29-7aed-4d70-ae66-c5983c45d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 79896 anomalies to result_set1.csv\n"
     ]
    }
   ],
   "source": [
    "detect_anomalies(ad1, output_file='result_set1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8576725-4d41-4770-8164-08dfa8e1249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 122659 anomalies to result_set2.csv\n"
     ]
    }
   ],
   "source": [
    "detect_anomalies(ad2, output_file='result_set2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
